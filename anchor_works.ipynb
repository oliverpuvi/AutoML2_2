{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for three interpretability techniques: LIME, ANCHOR, CIU\n",
    "\n",
    "Metrics tested are identity, separability, fidelity, and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "import metrics_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = \"datasets\"\n",
    "\n",
    "folder_names = []\n",
    "attribute_names_list = []\n",
    "categorical_indicator_list = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for folder_name in os.listdir(datasets_folder):\n",
    "    folder_path = os.path.join(datasets_folder, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        attribute_names_path = os.path.join(folder_path, \"attribute_names.csv\")\n",
    "        categorical_indicator_path = os.path.join(folder_path, \"categorical_indicator.csv\")\n",
    "        X_path = os.path.join(folder_path, \"X.csv\")\n",
    "        y_path = os.path.join(folder_path, \"y.csv\")\n",
    "        \n",
    "        attribute_names_df = pd.read_csv(attribute_names_path)\n",
    "        categorical_indicator_df = pd.read_csv(categorical_indicator_path)\n",
    "        X_df = pd.read_csv(X_path)\n",
    "        y_df = pd.read_csv(y_path)\n",
    "\n",
    "        unique_classes = y_df.iloc[:, 0].unique()\n",
    "        sampled_indices = []\n",
    "        for cls in unique_classes:\n",
    "            cls_indices = y_df[y_df.iloc[:, 0] == cls].index\n",
    "            sampled_indices.append(np.random.choice(cls_indices, 1)[0])\n",
    "\n",
    "        sampled_indices = np.array(sampled_indices)\n",
    "\n",
    "        needed_samples = 100 - len(sampled_indices)\n",
    "        seed_value = 42 \n",
    "        np.random.seed(seed_value)\n",
    "\n",
    "        if needed_samples > 0:\n",
    "            additional_indices = np.random.choice(y_df.index, needed_samples, replace=False)\n",
    "            sampled_indices = np.concatenate([sampled_indices, additional_indices])\n",
    "        \n",
    "        \n",
    "        X_list.append(X_df.loc[sampled_indices])\n",
    "        y_list.append(y_df.loc[sampled_indices])\n",
    "\n",
    "        folder_names.append(folder_name)\n",
    "        attribute_names_list.append(attribute_names_df)\n",
    "        categorical_indicator_list.append(categorical_indicator_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric_and_impute(X_list, y_list):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    def process_X_dataframe(df):\n",
    "        for column in df.columns:\n",
    "            if isinstance(df[column].iloc[0], csr_matrix):\n",
    "                df[column] = df[column].apply(lambda x: x.toarray()[0,0] if x.shape[1] == 1 else x.toarray())\n",
    "\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].fillna('Missing')\n",
    "                df[column] = label_encoder.fit_transform(df[column])\n",
    "            else:\n",
    "                if df[column].notna().any():\n",
    "                    df[column] = imputer.fit_transform(df[[column]]).ravel()\n",
    "                else:\n",
    "                    df[column] = df[column].fillna(0)\n",
    "        return df\n",
    "\n",
    "    def process_y_dataframe(df):\n",
    "        if df.dtypes[0] == 'object' or not np.issubdtype(df.dtypes[0], np.number):\n",
    "            df_encoded = df.apply(lambda x: label_encoder.fit_transform(x))\n",
    "            df_encoded = df_encoded.rename(columns={df_encoded.columns[0]: 'class'})\n",
    "            return df_encoded\n",
    "        \n",
    "        else:\n",
    "            return df\n",
    "            print('aaaah')\n",
    "\n",
    "\n",
    "        \n",
    "    X_list = [process_X_dataframe(df) for df in X_list]\n",
    "    y_list = [process_y_dataframe(df) for df in y_list]\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "X_list, y_list = convert_to_numeric_and_impute(X_list, y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dataset here, gen five later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 - Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_list[0], y_list[0], test_size=0.2, random_state=555)\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "print(f\"Dataset {0} - Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_within_class_variance(X, labels, centroids):\n",
    "    unique_labels = np.unique(labels)\n",
    "    variances = {label: ((X[labels == label] - centroids[label])**2).mean() for label in unique_labels}\n",
    "    total_variance = np.mean(list(variances.values()))\n",
    "    return total_variance\n",
    "\n",
    "#Calculate the distance (e.g., Euclidean distance) between the centroids of each pair of classes. \n",
    "#This measures how far apart the classes are from each other.\n",
    "def calculate_between_class_separation(centroids):\n",
    "    unique_labels = list(centroids.keys())\n",
    "    separations = []\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i+1, len(unique_labels)):\n",
    "            separation = np.linalg.norm(centroids[unique_labels[i]] - centroids[unique_labels[j]])\n",
    "            separations.append(separation)\n",
    "    avg_separation = np.mean(separations)\n",
    "    return avg_separation\n",
    "\n",
    "def calculate_separability(X, labels):\n",
    "    centroids = calculate_centroids(X, labels)\n",
    "    within_class_var = calculate_within_class_variance(X, labels, centroids)\n",
    "    between_class_sep = calculate_between_class_separation(centroids)\n",
    "    \n",
    "    if within_class_var == 0:  \n",
    "        return np.inf\n",
    "    \n",
    "    separability_score = between_class_sep / within_class_var\n",
    "    return separability_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:02<00:00,  3.14s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [01:07<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.606726408004761\n",
      "(85.0, 3, 20)\n",
      "(2, 20, 400, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = np.unique(y_train).tolist()\n",
    "attribute_names = X_train.columns.tolist()\n",
    "\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_names,\n",
    "    attribute_names,\n",
    "    X_train.values)\n",
    "\n",
    "def exp_fn_blk(xtest):\n",
    "    exp1 = []\n",
    "    for i in tqdm.tqdm(range(len(xtest))):\n",
    "        start_clock = time.time()\n",
    "        exp = explainer.explain_instance(X_test.values[i], rf.predict, threshold=0.95)\n",
    "        end_clock = time.time()\n",
    "        calc_time = end_clock - start_clock\n",
    "        exp_list = [0]*len(X_train.columns)\n",
    "        for j in exp.features():\n",
    "            exp_list[j] = 1\n",
    "        exp1.append(exp_list)\n",
    "    return np.array(exp1), calc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                         | 1/20 [00:06<01:54,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/joblib/parallel.py:825\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test))):\n\u001b[0;32m----> 4\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     prec \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mprecision()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(prec\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_test))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_tabular.py:278\u001b[0m, in \u001b[0;36mAnchorTabularExplainer.explain_instance\u001b[0;34m(self, data_row, classifier_fn, threshold, delta, tau, batch_size, max_anchor_size, desired_label, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m sample_fn, mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sample_fn(\n\u001b[1;32m    276\u001b[0m     data_row, classifier_fn, desired_label\u001b[38;5;241m=\u001b[39mdesired_label)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# return sample_fn, mapping\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43manchor_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnchorBaseBeam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_beam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesired_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_anchor_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_anchor_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_names_to_exp(data_row, exp, mapping)\n\u001b[1;32m    283\u001b[0m exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_row\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_base.py:314\u001b[0m, in \u001b[0;36mAnchorBaseBeam.anchor_beam\u001b[0;34m(sample_fn, delta, epsilon, batch_size, min_shared_samples, desired_confidence, beam_size, verbose, epsilon_stop, min_samples_start, max_anchor_size, verbose_every, stop_on_first, coverage_samples)\u001b[0m\n\u001b[1;32m    311\u001b[0m initial_stats \u001b[38;5;241m=\u001b[39m AnchorBaseBeam\u001b[38;5;241m.\u001b[39mget_initial_statistics(tuples,\n\u001b[1;32m    312\u001b[0m                                                       state)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# print tuples, beam_size\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m chosen_tuples \u001b[38;5;241m=\u001b[39m \u001b[43mAnchorBaseBeam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlucb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtuples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m best_of_size[current_size] \u001b[38;5;241m=\u001b[39m [tuples[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m chosen_tuples]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_base.py:107\u001b[0m, in \u001b[0;36mAnchorBaseBeam.lucb\u001b[0;34m(sample_fns, initial_stats, epsilon, delta, batch_size, top_n, verbose, verbose_every)\u001b[0m\n\u001b[1;32m    105\u001b[0m means[ut] \u001b[38;5;241m=\u001b[39m positives[ut] \u001b[38;5;241m/\u001b[39m n_samples[ut]\n\u001b[1;32m    106\u001b[0m n_samples[lt] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m--> 107\u001b[0m positives[lt] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msample_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m means[lt] \u001b[38;5;241m=\u001b[39m positives[lt] \u001b[38;5;241m/\u001b[39m n_samples[lt]\n\u001b[1;32m    109\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_base.py:201\u001b[0m, in \u001b[0;36mAnchorBaseBeam.get_sample_fns.<locals>.<lambda>\u001b[0;34m(n, t)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[0;32m--> 201\u001b[0m     sample_fns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mlambda\u001b[39;00m n, t\u001b[38;5;241m=\u001b[39mt: \u001b[43mcomplete_sample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample_fns\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_base.py:171\u001b[0m, in \u001b[0;36mAnchorBaseBeam.get_sample_fns.<locals>.complete_sample_fn\u001b[0;34m(t, n)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete_sample_fn\u001b[39m(t, n):\n\u001b[0;32m--> 171\u001b[0m     raw_data, data, labels \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     current_idx \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# idxs = range(state['data'].shape[0], state['data'].shape[0] + n)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_tabular.py:265\u001b[0m, in \u001b[0;36mAnchorTabularExplainer.get_sample_fn.<locals>.sample_fn\u001b[0;34m(present, num_samples, compute_labels)\u001b[0m\n\u001b[1;32m    263\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_labels:\n\u001b[0;32m--> 265\u001b[0m     labels \u001b[38;5;241m=\u001b[39m (\u001b[43mpredict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m true_label)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_data, data, labels\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/anchor/anchor_tabular.py:209\u001b[0m, in \u001b[0;36mAnchorTabularExplainer.get_sample_fn.<locals>.predict_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_fn\u001b[39m(x):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:630\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:683\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    680\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    681\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)]\n\u001b[1;32m    682\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m--> 683\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[1;32m    690\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/joblib/parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/joblib/parallel.py:856\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(islice), final_batch_size):\n\u001b[1;32m    852\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m BatchedCalls(islice[i:i \u001b[38;5;241m+\u001b[39m final_batch_size],\n\u001b[1;32m    853\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mget_nested_backend(),\n\u001b[1;32m    854\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reducer_callback,\n\u001b[1;32m    855\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickle_cache)\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# finally, get one task.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready_batches\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/queue.py:149\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m Full\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_put\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfinished_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/queue.py:213\u001b[0m, in \u001b[0;36mQueue._put\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_put\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241m.\u001b[39mappend(item)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "prec = 0\n",
    "for i in tqdm.tqdm(range(len(X_test))):\n",
    "    exp = explainer.explain_instance(X_test.values[i], rf.predict, threshold=0.95)\n",
    "    prec += exp.precision()\n",
    "print(prec/len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANCHOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0\n",
      "Dataset 307 - Accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:01<00:00,  3.08s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:54<00:00,  2.73s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [01:06<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1\n",
      "Dataset 1067 - Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:52<00:00,  2.61s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:47<00:00,  2.36s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:48<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2\n",
      "Dataset 50 - Accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 51.65it/s]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 53.44it/s]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 52.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3\n",
      "Dataset 32 - Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:17<00:00,  3.88s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [01:32<00:00,  4.60s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [01:30<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 4\n",
      "Dataset 1466 - Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:35<00:00,  4.79s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [01:36<00:00,  4.84s/it]\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:59<00:40,  5.10s/it]"
     ]
    }
   ],
   "source": [
    "# Store scores\n",
    "identity_anchor_scores = []\n",
    "separability_anchor_scores = []\n",
    "speed_anchor_seconds = []\n",
    "precision_scores = []\n",
    "\n",
    "df_interp = pd.DataFrame(columns=[\"Dataset\", \"Fidelity\", \"Identity\", \"Separability\", \"Speed\"])\n",
    "\n",
    "for i in range(len(X_list)):\n",
    "    print(f\"Dataset {i}\")\n",
    "    X, y = X_list[i], y_list[i].squeeze()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=555)\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "                np.unique(y_train).tolist(),\n",
    "                X_train.columns.tolist(),\n",
    "                X_train.values)\n",
    "    \n",
    "    \n",
    "\n",
    "    def exp_fn_blk(xtest):\n",
    "        exp1 = []\n",
    "        prec = 0\n",
    "        for i in tqdm.tqdm(range(len(xtest))):\n",
    "            start_clock = time.time()\n",
    "            exp = explainer.explain_instance(X_test.values[i], rf.predict, threshold=0.95)\n",
    "            end_clock = time.time()\n",
    "            prec += exp.precision()\n",
    "            calc_time = end_clock - start_clock\n",
    "            exp_list = [0]*len(X_train.columns)\n",
    "            for j in exp.features():\n",
    "                exp_list[j] = 1\n",
    "            exp1.append(exp_list)\n",
    "        return np.array(exp1), calc_time, prec\n",
    "\n",
    "    \n",
    "    start_time = time.time() \n",
    "    exp1 = exp_fn_blk(X_test[:100])\n",
    "    exp2 = exp_fn_blk(X_test[:100])\n",
    "    end_time = time.time()\n",
    "    speed = end_time - start_time \n",
    "    \n",
    "    prec = 0\n",
    "    for i in tqdm.tqdm(range(len(X_test))):\n",
    "        exp = explainer.explain_instance(X_test.values[i], rf.predict, threshold=0.95)\n",
    "        prec += exp.precision()\n",
    "    \n",
    "    \n",
    "    df_interp = df_interp.append({\n",
    "        \"Dataset\": i,\n",
    "        \"Fidelity\": prec, \n",
    "        \"Identity\": metrics_rules.calc_identity_rules(exp1[0], exp2[0]),\n",
    "        \"Separability\": metrics_rules.calc_separability_rules(exp1[0]),\n",
    "        \"Speed\": speed \n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp.columns = ['anchor_' + col for col in df_interp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp.to_csv('records_anchor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
