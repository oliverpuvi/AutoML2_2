{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4ff37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install skope-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fc7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py-ciu==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f5db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ciu import determine_ciu\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from skrules import SkopeRules\n",
    "import openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec78561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New proposed separability\n",
    "\n",
    "#calculate the centroid (mean vector) of the feature values. \n",
    "#This represents the average position of each class in the feature space\n",
    "def calculate_centroids(X, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    centroids = {label: X[labels == label].mean(axis=0) for label in unique_labels}\n",
    "    return centroids\n",
    "\n",
    "#Calculate the average variance within each class. \n",
    "#This measures how spread out each class is around its centroid.\n",
    "def calculate_within_class_variance(X, labels, centroids):\n",
    "    unique_labels = np.unique(labels)\n",
    "    variances = {label: ((X[labels == label] - centroids[label])**2).mean() for label in unique_labels}\n",
    "    total_variance = np.mean(list(variances.values()))\n",
    "    return total_variance\n",
    "\n",
    "#Calculate the distance (e.g., Euclidean distance) between the centroids of each pair of classes. \n",
    "#This measures how far apart the classes are from each other.\n",
    "def calculate_between_class_separation(centroids):\n",
    "    unique_labels = list(centroids.keys())\n",
    "    separations = []\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i+1, len(unique_labels)):\n",
    "            separation = np.linalg.norm(centroids[unique_labels[i]] - centroids[unique_labels[j]])\n",
    "            separations.append(separation)\n",
    "    avg_separation = np.mean(separations)\n",
    "    return avg_separation\n",
    "\n",
    "def calculate_separability(X, labels):\n",
    "    centroids = calculate_centroids(X, labels)\n",
    "    within_class_var = calculate_within_class_variance(X, labels, centroids)\n",
    "    between_class_sep = calculate_between_class_separation(centroids)\n",
    "    \n",
    "    if within_class_var == 0:  \n",
    "        return np.inf\n",
    "    \n",
    "    separability_score = between_class_sep / within_class_var\n",
    "    return separability_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2100d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_identity(exp1, exp2):\n",
    "    dis = np.array([np.array_equal(exp1[i], exp2[i]) for i in range(len(exp1))])\n",
    "    total = dis.shape[0]\n",
    "    true = np.sum(dis)\n",
    "    score = (total - true) / total\n",
    "    return score * 100, true, total\n",
    "\n",
    "def calc_stability(exp, labels):\n",
    "    total = labels.shape[0]\n",
    "    label_values = np.unique(labels)\n",
    "    n_clusters = label_values.shape[0]\n",
    "    init = np.array([[np.average(exp[np.where(labels == i)], axis = 0)] for i in label_values]).squeeze()\n",
    "    ct = sklearn.cluster.KMeans(n_clusters = n_clusters, random_state=1, n_init=10, init = init)\n",
    "    ct.fit(exp)\n",
    "    error = np.sum(np.abs(labels-ct.labels_))\n",
    "    if error/total > 0.5:\n",
    "        error = total-error\n",
    "    return error, total\n",
    "\n",
    "def enc_exp(exp, feature_num):\n",
    "    enc_exp = np.zeros((len(exp),feature_num))\n",
    "    for i in range(len(exp)):\n",
    "        for j in range(len(exp[i])):\n",
    "            enc_exp[i][int(exp[i,j,0])] = exp[i,j,1]\n",
    "    return enc_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a36d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(x, x_dash):\n",
    "    x = x.copy()\n",
    "    x_dash = x_dash.copy()\n",
    "    x_rand = np.random.random(x.shape[0])\n",
    "    x_new = [x[i] if x_rand[i] > 0.5 else x_dash[i] for i in range(len(x))]\n",
    "    x_dash_new = [x_dash[i] if x_rand[i] > 0.5 else x[i] for i in range(len(x))]\n",
    "    return x_new, x_dash_new\n",
    "\n",
    "def calc_trust_score(test_x, exp, m, feat_list, model):\n",
    "    total_recalls = []\n",
    "    for i in range(len(test_x)):\n",
    "        feat_score = np.zeros((len(feat_list)))\n",
    "        for _ in range(m):\n",
    "            x = test_x[i].copy()\n",
    "            x_dash = test_x[np.random.randint(0,len(test_x))].copy()\n",
    "            x_perm, x_dash_perm = permute(x, x_dash)\n",
    "            for j in range(len(feat_list)):\n",
    "                z = np.concatenate((x_perm[:j+1], x_dash_perm[j+1:]))\n",
    "                z_dash = np.concatenate((x_dash_perm[:j], x_perm[j:]))\n",
    "                p_z = model.predict_proba(np.array(z).reshape(1, -1))\n",
    "                p_z_dash = model.predict_proba(z_dash.reshape(1,-1))\n",
    "                feat_score[j] = feat_score[j] + np.linalg.norm(p_z-p_z_dash)\n",
    "        feat_score = feat_score/m\n",
    "        gold_feat_fs = np.argpartition(feat_score, -6)[-6:]\n",
    "        recall = len(set(exp[i][:6, 0]).intersection(set(gold_feat_fs)))/6\n",
    "        total_recalls.append(recall)\n",
    "    return np.mean(total_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3360d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = \"datasets\"\n",
    "\n",
    "folder_names = []\n",
    "attribute_names_list = []\n",
    "categorical_indicator_list = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for folder_name in os.listdir(datasets_folder):\n",
    "    folder_path = os.path.join(datasets_folder, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        attribute_names_path = os.path.join(folder_path, \"attribute_names.csv\")\n",
    "        categorical_indicator_path = os.path.join(folder_path, \"categorical_indicator.csv\")\n",
    "        X_path = os.path.join(folder_path, \"X.csv\")\n",
    "        y_path = os.path.join(folder_path, \"y.csv\")\n",
    "        \n",
    "        attribute_names_df = pd.read_csv(attribute_names_path)\n",
    "        categorical_indicator_df = pd.read_csv(categorical_indicator_path)\n",
    "        X_df = pd.read_csv(X_path)\n",
    "        y_df = pd.read_csv(y_path)\n",
    "\n",
    "        unique_classes = y_df.iloc[:, 0].unique()\n",
    "        sampled_indices = []\n",
    "        for cls in unique_classes:\n",
    "            cls_indices = y_df[y_df.iloc[:, 0] == cls].index\n",
    "            sampled_indices.append(np.random.choice(cls_indices, 1)[0])\n",
    "\n",
    "        sampled_indices = np.array(sampled_indices)\n",
    "\n",
    "        needed_samples = 100 - len(sampled_indices)\n",
    "        seed_value = 42 \n",
    "        np.random.seed(seed_value)\n",
    "\n",
    "        if needed_samples > 0:\n",
    "            additional_indices = np.random.choice(y_df.index, needed_samples, replace=False)\n",
    "            sampled_indices = np.concatenate([sampled_indices, additional_indices])\n",
    "        \n",
    "        \n",
    "        X_list.append(X_df.loc[sampled_indices])\n",
    "        y_list.append(y_df.loc[sampled_indices])\n",
    "\n",
    "        folder_names.append(folder_name)\n",
    "        attribute_names_list.append(attribute_names_df)\n",
    "        categorical_indicator_list.append(categorical_indicator_df)\n",
    "        \n",
    "        \n",
    "def convert_to_numeric_and_impute(X_list, y_list):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    def process_X_dataframe(df):\n",
    "        for column in df.columns:\n",
    "            if isinstance(df[column].iloc[0], csr_matrix):\n",
    "                df[column] = df[column].apply(lambda x: x.toarray()[0,0] if x.shape[1] == 1 else x.toarray())\n",
    "\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].fillna('Missing')\n",
    "                df[column] = label_encoder.fit_transform(df[column])\n",
    "            else:\n",
    "                if df[column].notna().any():\n",
    "                    df[column] = imputer.fit_transform(df[[column]]).ravel()\n",
    "                else:\n",
    "                    df[column] = df[column].fillna(0)\n",
    "        return df\n",
    "\n",
    "    def process_y_dataframe(df):\n",
    "        if df.dtypes[0] == 'object' or not np.issubdtype(df.dtypes[0], np.number):\n",
    "            df_encoded = df.apply(lambda x: label_encoder.fit_transform(x))\n",
    "            df_encoded = df_encoded.rename(columns={df_encoded.columns[0]: 'class'})\n",
    "            return df_encoded\n",
    "        \n",
    "        else:\n",
    "            return df\n",
    "            print('aaaah')\n",
    "\n",
    "\n",
    "        \n",
    "    X_list = [process_X_dataframe(df) for df in X_list]\n",
    "    y_list = [process_y_dataframe(df) for df in y_list]\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "X_list, y_list = convert_to_numeric_and_impute(X_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c051e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ciu import determine_ciu\n",
    "\n",
    "def exp_fn_ciu(xtest, model, X_train):\n",
    "    exp1 = []\n",
    "    for i in range(len(xtest)):\n",
    "        exp = determine_ciu(xtest.iloc[i:i+1], model.predict_proba, X_train.to_dict('list'), samples=1000, prediction_index=1)\n",
    "        exp_list = [[feat_list.index(i), exp.ci[i]] for i in exp.ci]\n",
    "        exp1.append(exp_list)\n",
    "    return np.array(exp1)\n",
    "\n",
    "def permute(x, x_dash):\n",
    "    x = x.copy()\n",
    "    x_dash = x_dash.copy()\n",
    "    x_rand = np.random.random(x.shape[0])\n",
    "    x_new = [x[i] if x_rand[i] > 0.5 else x_dash[i] for i in range(len(x))]\n",
    "    x_dash_new = [x_dash[i] if x_rand[i] > 0.5 else x[i] for i in range(len(x))]\n",
    "    return x_new, x_dash_new\n",
    "\n",
    "def calc_trust_score(test_x, exp, m, feat_list, model):\n",
    "    total_recalls = []\n",
    "    for i in range(len(test_x)):\n",
    "        feat_score = np.zeros((len(feat_list)))\n",
    "        for _ in range(m):\n",
    "            x = test_x[i].copy()\n",
    "            x_dash = test_x[np.random.randint(0,len(test_x))].copy()\n",
    "            x_perm, x_dash_perm = permute(x, x_dash)\n",
    "            for j in range(len(feat_list)):\n",
    "                z = np.concatenate((x_perm[:j+1], x_dash_perm[j+1:]))\n",
    "                z_dash = np.concatenate((x_dash_perm[:j], x_perm[j:]))\n",
    "                z = np.array(z).reshape(1, -1)\n",
    "                z_dash = np.array(z_dash).reshape(1, -1)\n",
    "                \n",
    "                p_z = model.predict_proba(z)\n",
    "                p_z_dash = model.predict_proba(z_dash)\n",
    "                feat_score[j] = feat_score[j] + np.linalg.norm(p_z-p_z_dash)\n",
    "        feat_score = feat_score/m\n",
    "        gold_feat_fs = np.argpartition(feat_score, -6)[-6:]\n",
    "        recall = len(set(exp[i][:6, 0]).intersection(set(gold_feat_fs)))/6\n",
    "        total_recalls.append(recall)\n",
    "    return np.mean(total_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a427f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_ciu_as_prediction(ciu_result, threshold=0.5):\n",
    "    # Assuming ciu_result is a list of tuples (feature, importance)\n",
    "    # And that a higher cumulative importance suggests a particular class (e.g., class 1)\n",
    "    cumulative_importance = sum(importance for feature, importance in ciu_result)\n",
    "    return 1 if cumulative_importance > threshold else 0\n",
    "\n",
    "\n",
    "def calculate_fidelity_score(X_test, model, ciu_results):\n",
    "    model_predictions = model.predict(X_test)\n",
    "    ciu_predictions = [interpret_ciu_as_prediction(ciu_result) for ciu_result in ciu_results]\n",
    "    correct_predictions = sum(ciu_pred == model_pred for ciu_pred, model_pred in zip(ciu_predictions, model_predictions))\n",
    "    fidelity_score = correct_predictions / len(X_test)\n",
    "    return fidelity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 307 - Accuracy: 0.55\n",
      "Identity score is 0.0\n",
      "Separability score is 8.673951444138323\n",
      "Speed: 2.39\n",
      "fidelity: 0.15\n",
      "Dataset 1067 - Accuracy: 0.85\n",
      "Identity score is 40.0\n",
      "Separability score is 0.0003532835925087861\n",
      "Speed: 4.37\n",
      "fidelity: 0.15\n",
      "Dataset 50 - Accuracy: 0.65\n",
      "Identity score is 100.0\n",
      "Separability score is inf\n",
      "Speed: 1.34\n",
      "fidelity: 0.0\n",
      "Dataset 32 - Accuracy: 0.8\n",
      "Identity score is 0.0\n",
      "Separability score is 0.3330128424502104\n",
      "Speed: 3.27\n",
      "fidelity: 0.0\n",
      "Dataset 1466 - Accuracy: 0.95\n",
      "Identity score is 0.0\n",
      "Separability score is 0.017689787072479875\n",
      "Speed: 10.42\n",
      "fidelity: 0.3\n",
      "Dataset 1459 - Accuracy: 0.25\n",
      "Identity score is 5.0\n",
      "Separability score is 0.1937284183100717\n",
      "Speed: 1.3\n",
      "fidelity: 0.2\n",
      "Dataset 1050 - Accuracy: 0.9\n",
      "Identity score is 80.0\n",
      "Separability score is 5.317486448639185e-06\n",
      "Speed: 10.94\n",
      "fidelity: 0.0\n",
      "Dataset 1068 - Accuracy: 0.9\n",
      "Identity score is 60.0\n",
      "Separability score is 0.00011024498207074123\n",
      "Speed: 4.63\n",
      "fidelity: 0.0\n",
      "Dataset 1467 - Accuracy: 0.9\n",
      "Identity score is 10.0\n",
      "Separability score is 0.1996678572896105\n",
      "Speed: 4.27\n",
      "fidelity: 0.0\n",
      "Dataset 40496 - Accuracy: 0.75\n",
      "Identity score is 0.0\n",
      "Separability score is 18.117787000094467\n",
      "Speed: 1.47\n",
      "fidelity: 0.1\n",
      "Dataset 470 - Accuracy: 0.55\n",
      "Identity score is 0.0\n",
      "Separability score is 0.09603377552027106\n",
      "Speed: 1.56\n",
      "fidelity: 0.35\n",
      "Dataset 23381 - Accuracy: 0.65\n",
      "Identity score is 0.0\n",
      "Separability score is 0.18107347770469506\n",
      "Speed: 2.24\n",
      "fidelity: 0.8\n",
      "Dataset 23380 - Accuracy: 0.45\n",
      "Identity score is 5.0\n",
      "Separability score is 1.0989533127255964\n",
      "Speed: 10.61\n",
      "fidelity: 0.15\n",
      "Dataset 18 - Accuracy: 0.65\n",
      "Identity score is 0.0\n",
      "Separability score is 0.017777424754352955\n",
      "Speed: 1.21\n",
      "fidelity: 0.15\n",
      "Dataset 1480 - Accuracy: 0.85\n",
      "Identity score is 55.00000000000001\n",
      "Separability score is 0.02529010048257055\n",
      "Speed: 1.7\n",
      "fidelity: 0.85\n",
      "Dataset 1489 - Accuracy: 0.9\n",
      "Identity score is 5.0\n",
      "Separability score is 1.354182167783788\n",
      "Speed: 1.1\n",
      "fidelity: 0.8\n",
      "Dataset 11 - Accuracy: 0.7\n",
      "Identity score is 0.0\n",
      "Separability score is 1.1092215124714584\n",
      "Speed: 0.81\n",
      "fidelity: 0.3\n",
      "Dataset 29 - Accuracy: 0.65\n",
      "Identity score is 65.0\n",
      "Separability score is 0.0009209904873366024\n",
      "Speed: 3.06\n",
      "fidelity: 0.5\n",
      "Dataset 42 - Accuracy: 0.1\n",
      "Identity score is 100.0\n",
      "Separability score is inf\n",
      "Speed: 13.06\n",
      "fidelity: 0.0\n",
      "Dataset 4538 - Accuracy: 0.4\n",
      "Identity score is 75.0\n",
      "Separability score is 403.38654775562793\n",
      "Speed: 9.73\n",
      "fidelity: 0.0\n",
      "Dataset 6 - Accuracy: 0.5\n",
      "Identity score is 0.0\n",
      "Separability score is 4.111033254073387\n",
      "Speed: 4.2\n",
      "fidelity: 0.0\n",
      "Dataset 188 - Accuracy: 0.4\n",
      "Identity score is 0.0\n",
      "Separability score is 0.014781729675088555\n",
      "Speed: 4.51\n",
      "fidelity: 0.0\n",
      "Dataset 38 - Accuracy: 0.95\n",
      "Identity score is 10.0\n",
      "Separability score is 0.5829258814348668\n",
      "Speed: 7.74\n",
      "fidelity: 0.05\n",
      "Dataset 36 - Accuracy: 0.9\n",
      "Identity score is 15.0\n",
      "Separability score is 0.4146251496380936\n",
      "Speed: 4.17\n",
      "fidelity: 0.2\n",
      "Dataset 31 - Accuracy: 0.8\n",
      "Identity score is 10.0\n",
      "Separability score is 0.002082028882871328\n",
      "Speed: 4.37\n",
      "fidelity: 0.9\n",
      "Dataset 1462 - Accuracy: 0.95\n",
      "Identity score is 5.0\n",
      "Separability score is 0.49840058989779895\n",
      "Speed: 0.67\n",
      "fidelity: 0.55\n",
      "Dataset 1063 - Accuracy: 0.85\n",
      "Identity score is 35.0\n",
      "Separability score is 7.978272019988945e-05\n",
      "Speed: 5.02\n",
      "fidelity: 0.2\n",
      "Dataset 54 - Accuracy: 0.8\n",
      "Identity score is 0.0\n",
      "Separability score is 0.07758790193914546\n",
      "Speed: 4.03\n",
      "fidelity: 0.2\n",
      "Dataset 335 - Accuracy: 0.9\n",
      "Identity score is 0.0\n",
      "Separability score is 2.282020869002572\n",
      "Speed: 1.03\n",
      "fidelity: 0.55\n",
      "Dataset 1497 - Accuracy: 0.95\n",
      "Identity score is 45.0\n",
      "Separability score is 3.0298200800527813\n",
      "Speed: 6.08\n",
      "fidelity: 0.45\n",
      "Dataset 1464 - Accuracy: 0.65\n",
      "Identity score is 0.0\n",
      "Separability score is 0.001886928176901253\n",
      "Speed: 0.8\n",
      "fidelity: 0.7\n",
      "Dataset 37 - Accuracy: 0.7\n",
      "Identity score is 0.0\n",
      "Separability score is 0.03652023667572094\n",
      "Speed: 1.68\n",
      "fidelity: 0.3\n",
      "Dataset 334 - Accuracy: 0.6\n",
      "Identity score is 0.0\n",
      "Separability score is 0.5607198066138069\n",
      "Speed: 1.03\n",
      "fidelity: 0.15\n",
      "Dataset 333 - Accuracy: 0.65\n",
      "Identity score is 0.0\n",
      "Separability score is 1.423932918710342\n",
      "Speed: 1.13\n",
      "fidelity: 0.6\n",
      "Dataset 1053 - Accuracy: 0.95\n",
      "Identity score is 50.0\n",
      "Separability score is 3.5830692257303636e-06\n",
      "Speed: 4.58\n",
      "fidelity: 0.1\n",
      "Dataset 1120 - Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "ciu_identity_scores = []\n",
    "ciu_lime_separability_scores = []\n",
    "ciu_speed_scores = []\n",
    "ciu_fidelity_scores = []\n",
    "\n",
    "df_interp = pd.DataFrame(columns=[\"Dataset\", \"Fidelity\", \"Identity\", \"Separability\", \"Speed\"])\n",
    "\n",
    "\n",
    "for i in range(len(X_list)):\n",
    "    X, y = X_list[i], y_list[i].squeeze()  # Ensure y is a 1D array\n",
    "    \n",
    "    # Convert X and y to numeric if not already\n",
    "    \n",
    "    #Calculate overall separability\n",
    "    ciu_lime_separability = calculate_separability(X, y)\n",
    "    ciu_lime_separability_scores.append(ciu_lime_separability)\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=555)\n",
    "    feat_list = X_train.columns.tolist()\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Dataset {folder_names[i]} - Accuracy: {accuracy_score(y_test, model.predict(X_test))}\")\n",
    "    \n",
    "    #Applying bulk CIU\n",
    "    start_time = time.time()\n",
    "    bulk_ciu_result = exp_fn_ciu(X_test, model, X_train)\n",
    "    end_time = time.time()\n",
    "    ciu_speed = end_time - start_time\n",
    "    ciu_speed_scores.append(ciu_speed)\n",
    "    bulk_ciu_result2 = exp_fn_ciu(X_test, model, X_train)\n",
    "    \n",
    "    #Calculating identity score\n",
    "    ciu_identity = calc_identity(bulk_ciu_result, bulk_ciu_result2)\n",
    "    ciu_identity_scores.append(ciu_identity[0])\n",
    "    bulk_ciu_result = exp_fn_ciu(X_test, model, X_train)\n",
    "    \n",
    "    # Calculating fidelity scores\n",
    "    ciu_fidelity = calculate_fidelity_score(X_test, model, bulk_ciu_result)\n",
    "    ciu_fidelity_scores.append(ciu_fidelity)\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "        \"Dataset\": i,\n",
    "        \"Fidelity\": ciu_fidelity,\n",
    "        \"Identity\": ciu_identity,\n",
    "        \"Separability\": ciu_lime_separability,\n",
    "        \"Speed\": ciu_speed\n",
    "    }\n",
    "    \n",
    "    df_interp = df_interp.append(results, ignore_index=True)\n",
    "    \n",
    "    print(f\"Identity score is {ciu_identity[0]}\")\n",
    "    print(f\"Separability score is {ciu_lime_separability}\")\n",
    "\n",
    "    print(f\"Speed: {round(ciu_speed, 2)}\")\n",
    "    print(f\"fidelity: {ciu_fidelity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76857715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_t = pd.concat([\n",
    "#    pd.Series(ciu_lime_separability_scores, name='Separability_scores'),\n",
    "#    pd.Series(ciu_identity_scores, name='CIU_identity_scores'),\n",
    "#    pd.Series(ciu_speed_scores, name='CIU_speed_scores')\n",
    "#], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp.to_csv('records_ciu.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
