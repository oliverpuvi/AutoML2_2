{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for three interpretability techniques: LIME, ANCHOR, CIU\n",
    "\n",
    "Metrics tested are identity, stability, separability ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = \"datasets\"\n",
    "\n",
    "folder_names = []\n",
    "attribute_names_list = []\n",
    "categorical_indicator_list = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for folder_name in os.listdir(datasets_folder):\n",
    "    folder_path = os.path.join(datasets_folder, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        attribute_names_path = os.path.join(folder_path, \"attribute_names.csv\")\n",
    "        categorical_indicator_path = os.path.join(folder_path, \"categorical_indicator.csv\")\n",
    "        X_path = os.path.join(folder_path, \"X.csv\")\n",
    "        y_path = os.path.join(folder_path, \"y.csv\")\n",
    "        \n",
    "        attribute_names_df = pd.read_csv(attribute_names_path)\n",
    "        categorical_indicator_df = pd.read_csv(categorical_indicator_path)\n",
    "        X_df = pd.read_csv(X_path)\n",
    "        y_df = pd.read_csv(y_path)\n",
    "\n",
    "        unique_classes = y_df.iloc[:, 0].unique()\n",
    "        sampled_indices = []\n",
    "        for cls in unique_classes:\n",
    "            cls_indices = y_df[y_df.iloc[:, 0] == cls].index\n",
    "            sampled_indices.append(np.random.choice(cls_indices, 1)[0])\n",
    "\n",
    "        sampled_indices = np.array(sampled_indices)\n",
    "\n",
    "        needed_samples = 100 - len(sampled_indices)\n",
    "        seed_value = 42 \n",
    "        np.random.seed(seed_value)\n",
    "\n",
    "        if needed_samples > 0:\n",
    "            additional_indices = np.random.choice(y_df.index, needed_samples, replace=False)\n",
    "            sampled_indices = np.concatenate([sampled_indices, additional_indices])\n",
    "        \n",
    "        \n",
    "        X_list.append(X_df.loc[sampled_indices])\n",
    "        y_list.append(y_df.loc[sampled_indices])\n",
    "\n",
    "        folder_names.append(folder_name)\n",
    "        attribute_names_list.append(attribute_names_df)\n",
    "        categorical_indicator_list.append(categorical_indicator_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric_and_impute(X_list, y_list):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    def process_X_dataframe(df):\n",
    "        for column in df.columns:\n",
    "            if isinstance(df[column].iloc[0], csr_matrix):\n",
    "                df[column] = df[column].apply(lambda x: x.toarray()[0,0] if x.shape[1] == 1 else x.toarray())\n",
    "\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].fillna('Missing')\n",
    "                df[column] = label_encoder.fit_transform(df[column])\n",
    "            else:\n",
    "                if df[column].notna().any():\n",
    "                    df[column] = imputer.fit_transform(df[[column]]).ravel()\n",
    "                else:\n",
    "                    df[column] = df[column].fillna(0)\n",
    "        return df\n",
    "\n",
    "    def process_y_dataframe(df):\n",
    "        if df.dtypes[0] == 'object' or not np.issubdtype(df.dtypes[0], np.number):\n",
    "            df_encoded = df.apply(lambda x: label_encoder.fit_transform(x))\n",
    "            df_encoded = df_encoded.rename(columns={df_encoded.columns[0]: 'class'})\n",
    "            return df_encoded\n",
    "        \n",
    "        else:\n",
    "            return df\n",
    "            print('aaaah')\n",
    "\n",
    "\n",
    "        \n",
    "    X_list = [process_X_dataframe(df) for df in X_list]\n",
    "    y_list = [process_y_dataframe(df) for df in y_list]\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "X_list, y_list = convert_to_numeric_and_impute(X_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_fidelity(instance, black_box_model, lime_explanation, num_samples=100):\n",
    "    num_features = len(instance)\n",
    "    perturbed_samples = generate_perturbed_samples(instance, num_features, num_samples)\n",
    "    bb_predictions = black_box_model.predict_proba(perturbed_samples)\n",
    "\n",
    "    class_idx = np.argmax(bb_predictions[0])\n",
    "    lime_weights = lime_explanation.as_list(label=class_idx)\n",
    "\n",
    "    lime_predictions = approximate_lime_predictions(lime_weights, perturbed_samples)\n",
    "\n",
    "    fidelity = accuracy_score(np.argmax(bb_predictions, axis=1), (lime_predictions > 0.5).astype(int))\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "def calc_identity(exp1, exp2):\n",
    "    dis = np.array([np.array_equal(exp1[i],exp2[i]) for i in range(len(exp1))])\n",
    "    total = dis.shape[0]\n",
    "    true = np.sum(dis)\n",
    "    score = (total-true)/total\n",
    "    return score*100, true, total\n",
    "\n",
    "def calc_separability(exp):\n",
    "    wrong = 0\n",
    "    for i in range(exp.shape[0]):\n",
    "        for j in range(exp.shape[0]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            eq = np.array_equal(exp[i],exp[j])\n",
    "            if eq:\n",
    "                wrong = wrong + 1\n",
    "    total = exp.shape[0]\n",
    "    score = 100*abs(wrong)/total**2\n",
    "    return wrong,total,total**2,score\n",
    "\n",
    "def calc_stability(exp, labels):\n",
    "    total = labels.shape[0]\n",
    "    label_values = np.unique(labels)\n",
    "    n_clusters = label_values.shape[0]\n",
    "    init = np.array([[np.average(exp[np.where(labels == i)], axis = 0)] for i in label_values]).squeeze()\n",
    "    ct = sklearn.cluster.KMeans(n_clusters = n_clusters, random_state=42)\n",
    "    ct.fit(exp)\n",
    "    error = np.sum(np.abs(labels-ct.labels_))\n",
    "    if error/total > 0.5:\n",
    "        error = total-error\n",
    "    return error, total\n",
    "\n",
    "\n",
    "def normalize_test(X_train, X_test):\n",
    "    X_test_norm = X_test.copy()\n",
    "    for i in X_train.columns:\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        scaler.fit(X_train[i].values.reshape(-1,1))\n",
    "        X_test_norm[i] = scaler.transform(X_test[i].values.reshape(-1,1))\n",
    "\n",
    "    return X_test_norm\n",
    "\n",
    "def calc_similarity(exp, X_test_norm):\n",
    "    dbscan = sklearn.cluster.DBSCAN(eps=0.5, min_samples=10)\n",
    "    dbscan.fit(X_test_norm[:400])\n",
    "    labels = dbscan.labels_\n",
    "    mean_dist = []\n",
    "    for i in np.unique(labels):\n",
    "        mean_dist.append(np.mean(sklearn.metrics.pairwise_distances(exp[np.where(labels == i), :, 1].squeeze(), metric='euclidean')))\n",
    "    return np.min(mean_dist)\n",
    "\n",
    "def permute(x, x_dash):\n",
    "    x = x.copy()\n",
    "    x_dash = x_dash.copy()\n",
    "    x_rand = np.random.random(x.shape[0])\n",
    "    x_new = [x[i] if x_rand[i] > 0.5 else x_dash[i] for i in range(len(x))]\n",
    "    x_dash_new = [x_dash[i] if x_rand[i] > 0.5 else x[i] for i in range(len(x))]\n",
    "    return x_new, x_dash_new\n",
    "\n",
    "def calc_trust_score(test_x, exp, m, feat_list):\n",
    "    total_recalls = []\n",
    "    for i in range(len(test_x)):\n",
    "        feat_score = np.zeros((len(feat_list)))\n",
    "        for _ in range(m):\n",
    "            x = test_x[i].copy()\n",
    "            x_dash = test_x[np.random.randint(0,len(test_x))].copy()\n",
    "            x_perm, x_dash_perm = permute(x, x_dash)\n",
    "            for j in range(len(feat_list)):\n",
    "                z = np.concatenate((x_perm[:j+1], x_dash_perm[j+1:]))\n",
    "                z_dash = np.concatenate((x_dash_perm[:j], x_perm[j:]))\n",
    "                p_z = model.predict_proba(z.reshape(1,-1))\n",
    "                p_z_dash = model.predict_proba(z_dash.reshape(1,-1))\n",
    "                feat_score[j] = feat_score[j] + np.linalg.norm(p_z-p_z_dash)\n",
    "        feat_score = feat_score/m\n",
    "        gold_feat_fs = np.argpartition(feat_score, -6)[-6:]\n",
    "        recall = len(set(exp[i][:6, 0]).intersection(set(gold_feat_fs)))/6\n",
    "        total_recalls.append(recall)\n",
    "    return np.mean(total_recalls)\n",
    "\n",
    "\n",
    "def approximate_lime_predictions(lime_weights, perturbed_samples):\n",
    "    lime_predictions = np.zeros(len(perturbed_samples))\n",
    "    for i, sample in enumerate(perturbed_samples):\n",
    "        prediction = 0\n",
    "        for feature_index, weight in lime_weights:\n",
    "            prediction += sample[feature_index] * weight\n",
    "        lime_predictions[i] = prediction\n",
    "    return lime_predictions\n",
    "\n",
    "\n",
    "def apply_lime_model_to_test_set(X_test, explanation, num_classes):\n",
    "    lime_weights = explanation.local_exp\n",
    "    lime_model_predictions = np.zeros((X_test.shape[0], num_classes))\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for class_idx in range(num_classes):\n",
    "            class_weights = dict(lime_weights[class_idx])\n",
    "            prediction = sum(X_test.iloc[i][feature] * weight for feature, weight in class_weights.items())\n",
    "            lime_model_predictions[i, class_idx] = prediction\n",
    "\n",
    "    return np.argmax(lime_model_predictions, axis=1)\n",
    "\n",
    "def generate_perturbed_samples(instance, num_features, num_samples=100):\n",
    "    perturbed_samples = []\n",
    "    for _ in range(num_samples):\n",
    "        perturbed_instance = instance.copy()\n",
    "        for feature in range(num_features):\n",
    "            perturbation = np.random.normal(0, 0.01)\n",
    "            perturbed_instance[feature] += perturbation\n",
    "        perturbed_samples.append(perturbed_instance)\n",
    "    return np.array(perturbed_samples)\n",
    "\n",
    "def approximate_lime_predictions(lime_weights, perturbed_samples, num_classes):\n",
    "    lime_predictions = np.zeros((len(perturbed_samples), num_classes))\n",
    "    for i, sample in enumerate(perturbed_samples):\n",
    "        for class_idx in range(num_classes):\n",
    "            if class_idx in lime_weights:\n",
    "                class_weights = dict(lime_weights[class_idx])\n",
    "                prediction = sum(sample[feature] * weight for feature, weight in class_weights.items())\n",
    "                lime_predictions[i, class_idx] = prediction\n",
    "    return lime_predictions\n",
    "\n",
    "def calc_lime_model_accuracy(X_test, y_test, model, lime_explainer, num_classes, num_samples=100):\n",
    "    lime_accuracies = []\n",
    "    for index, instance in enumerate(X_test.values):\n",
    "        try:\n",
    "            explanation = lime_explainer.explain_instance(instance, model.predict_proba, num_features=len(X_test.columns))\n",
    "            perturbed_samples = generate_perturbed_samples(instance, len(X_test.columns), num_samples)\n",
    "            lime_weights = explanation.local_exp\n",
    "            lime_predictions = approximate_lime_predictions(lime_weights, perturbed_samples, num_classes)\n",
    "            predicted_classes = np.argmax(lime_predictions, axis=1)\n",
    "            lime_accuracy = accuracy_score(y_test.iloc[index:index+1].repeat(num_samples), predicted_classes)\n",
    "            lime_accuracies.append(lime_accuracy)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in LIME model accuracy calculation for instance {index}: {e}\")\n",
    "            lime_accuracies.append(None)\n",
    "    return np.nanmean(lime_accuracies)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0\n",
      "Unique classes in y: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Fidelity: 0.0\n",
      "identity (100.0, 0, 20)\n",
      "separability (0, 20, 400, 0.0)\n",
      "trust score 0.4666666666666666\n",
      "Dataset 1\n",
      "Unique classes in y: [0 1]\n",
      "Fidelity: 0.6\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_interp = pd.DataFrame(columns=[\"Dataset\", \"Fidelity\", \"Identity\", \"Separability\", \"Speed\"]) #similarity vb lisada\n",
    "\n",
    "\n",
    "for i in range(len(X_list)):\n",
    "    print(f\"Dataset {i}\")\n",
    "    X, y = X_list[i], y_list[i].squeeze()\n",
    "\n",
    "    # Check the number of unique classes\n",
    "    unique_classes = np.unique(y)\n",
    "    print(f\"Unique classes in y: {unique_classes}\")\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=555)\n",
    "    feat_list = X_train.columns.tolist()\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    test_x = X_test.values\n",
    "    class_names = unique_classes.tolist()\n",
    "    num_classes = len(unique_classes)\n",
    "\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X_train.values, \n",
    "        feature_names=[col.replace(' ', '_') for col in X_train.columns], \n",
    "        class_names=class_names, \n",
    "        discretize_continuous=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.time() \n",
    "    lime_accuracy = calc_lime_model_accuracy(X_test, y_test, model, lime_explainer, num_classes)\n",
    "    end_time = time.time()\n",
    "    speed = end_time - start_time \n",
    "\n",
    "    print(f\"Fidelity: {lime_accuracy}\")\n",
    "\n",
    "    def exp_fn(index):\n",
    "        instance = test_x[index]\n",
    "        def predict_fn(x):\n",
    "            proba = model.predict_proba(x)\n",
    "            return proba\n",
    "\n",
    "        explanation = lime_explainer.explain_instance(instance, predict_fn, num_features=len(feat_list))\n",
    "        predicted_class = model.predict(instance.reshape(1, -1))\n",
    "        return explanation\n",
    "\n",
    "    def exp_fn_blk(xtest, exp_fn):\n",
    "        exp1 = []\n",
    "        for d in range(min(100, len(xtest))):  \n",
    "            exp = exp_fn(d)\n",
    "            available_labels = exp.available_labels()\n",
    "            if len(available_labels) > 0:\n",
    "                exp1.append(exp.as_map()[available_labels[0]])\n",
    "            else:\n",
    "                exp1.append(None)  \n",
    "        return np.array(exp1)\n",
    "\n",
    "    exp_fn_wrap = lambda x: np.array(exp_fn_blk(x, exp_fn))\n",
    "    \n",
    "    try:\n",
    "        exp1 = exp_fn_blk(test_x, exp_fn)\n",
    "        exp2 = exp_fn_blk(test_x, exp_fn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing dataset {i}: {e}\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    ident = calc_identity(exp1,exp2)\n",
    "    print('identity', ident)\n",
    "\n",
    "    s = calc_separability(test_x[:100])\n",
    "    print('separability', s)\n",
    "\n",
    "    def enc_exp(exp, feature_num):\n",
    "        enc_exp = np.zeros((len(exp), feature_num))\n",
    "        for s in range(len(exp)):\n",
    "            for j in range(len(exp[s])):\n",
    "                feature_index = int(exp[s, j, 0])\n",
    "                if feature_index < feature_num:\n",
    "                    enc_exp[s][feature_index] = exp[s, j, 1]\n",
    "        return enc_exp\n",
    "\n",
    "    enc1 = enc_exp(exp1, len(feat_list))\n",
    "    #sb = calc_stability(enc1, y_test[:100])\n",
    "    #print('stability', sb)\n",
    "    \n",
    "    X_test_norm = normalize_test(X_train, X_test)\n",
    "    #sim = calc_similarity(exp1, X_test_norm[:100])\n",
    "    #print('similarity', sim)\n",
    "    \n",
    "    trust_score = calc_trust_score(test_x[:100], exp1, 5, feat_list)\n",
    "    print('trust score', trust_score)\n",
    "    \n",
    "    df_interp = df_interp.append({\n",
    "        \"Dataset\": i,\n",
    "        \"Fidelity\": lime_accuracy,\n",
    "        \"Identity\": ident,\n",
    "        \"Separability\": s,\n",
    "        #\"Similarity\": sim,\n",
    "        \"Speed\": speed  \n",
    "    }, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp.to_csv('records_lime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
