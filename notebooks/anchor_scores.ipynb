{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANCHOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "import metrics_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the datasets folder\n",
    "datasets_folder = \"../datasets\"\n",
    "\n",
    "# Initialize empty lists to store dataframes for each file\n",
    "folder_names = []\n",
    "attribute_names_list = []\n",
    "categorical_indicator_list = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Loop through each folder in the datasets folder\n",
    "for folder_name in os.listdir(datasets_folder):\n",
    "    folder_path = os.path.join(datasets_folder, folder_name)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Construct file paths for each CSV file in the folder\n",
    "        attribute_names_path = os.path.join(folder_path, \"attribute_names.csv\")\n",
    "        categorical_indicator_path = os.path.join(folder_path, \"categorical_indicator.csv\")\n",
    "        X_path = os.path.join(folder_path, \"X.csv\")\n",
    "        y_path = os.path.join(folder_path, \"y.csv\")\n",
    "        \n",
    "        # Read each CSV file into a pandas dataframe\n",
    "        attribute_names_df = pd.read_csv(attribute_names_path)\n",
    "        categorical_indicator_df = pd.read_csv(categorical_indicator_path)\n",
    "        X_df = pd.read_csv(X_path)\n",
    "        y_df = pd.read_csv(y_path)\n",
    "        \n",
    "        # Append dataframes to the lists\n",
    "        attribute_names_list.append(attribute_names_df)\n",
    "        categorical_indicator_list.append(categorical_indicator_df)\n",
    "        X_list.append(X_df)\n",
    "        y_list.append(y_df)\n",
    "\n",
    "        # Save folder name to list\n",
    "        folder_names.append(folder_name)\n",
    "\n",
    "# Subsetting for less expensive runs\n",
    "X_list = [df.head(50) for df in X_list]\n",
    "y_list = [df.head(50) for df in y_list]\n",
    "\n",
    "# For testing the techniques\n",
    "X = X_list[:40]\n",
    "y = y_list[:40]\n",
    "\n",
    "# Names of chosen datasets\n",
    "X_folder_names = folder_names[:40]\n",
    "\n",
    "# For testing later\n",
    "X_list_test = X_list[-10:]\n",
    "y_list_test = y_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def convert_to_numeric_and_impute(X_list, y_list):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    def process_dataframe(df):\n",
    "        for column in df.columns:\n",
    "            if isinstance(df[column].iloc[0], csr_matrix):\n",
    "                df[column] = df[column].apply(lambda x: x.toarray()[0,0] if x.shape[1] == 1 else x.toarray())\n",
    "\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "            if df[column].dtype == 'object':\n",
    "                # Fill NaN with a placeholder and then label encode\n",
    "                df[column] = df[column].fillna('Missing')\n",
    "                df[column] = label_encoder.fit_transform(df[column])\n",
    "            else:\n",
    "                if df[column].notna().any():\n",
    "                    df[column] = imputer.fit_transform(df[[column]]).ravel()\n",
    "                else:\n",
    "                    df[column] = df[column].fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "    X_list = [process_dataframe(df) for df in X_list]\n",
    "    y_list = [process_dataframe(df) for df in y_list]\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "X, y = convert_to_numeric_and_impute(X, y)\n",
    "X_list_test, y_list_test = convert_to_numeric_and_impute(X_list_test, y_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1046 - Accuracy: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1049 - Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.81s/it]\n",
      "100%|██████████| 10/10 [00:28<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1050 - Accuracy: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store scores\n",
    "identity_anchor_scores = []\n",
    "separability_anchor_scores = []\n",
    "speed_anchor_seconds = []\n",
    "# precision_scores = []\n",
    "accuracy_scores = []\n",
    "dataset_indeces = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.2, random_state=555)\n",
    "\n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_estimators=50, n_jobs=5)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "    print(f\"Dataset {folder_names[i]} - Accuracy: {accuracy}\")\n",
    "    dataset_indeces.append(folder_names[i])\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    np.unique(y_train).tolist(),\n",
    "    X_train.columns.tolist(),\n",
    "    X_train.values)\n",
    "\n",
    "    def exp_fn_blk(xtest):\n",
    "        exp1 = []\n",
    "        prec = 0\n",
    "        for i in tqdm.tqdm(range(len(xtest))):\n",
    "            start_clock = time.time()\n",
    "            exp = explainer.explain_instance(X_test.values[i], rf.predict, threshold=0.95)\n",
    "            end_clock = time.time()\n",
    "            prec += exp.precision()\n",
    "            calc_time = end_clock - start_clock\n",
    "            exp_list = [0]*len(X_train.columns)\n",
    "            for j in exp.features():\n",
    "                exp_list[j] = 1\n",
    "            exp1.append(exp_list)\n",
    "        return np.array(exp1), calc_time, prec\n",
    "\n",
    "    exp1 = exp_fn_blk(X_test)\n",
    "    exp2 = exp_fn_blk(X_test)\n",
    "\n",
    "    # precision_scores.append(exp1[2]/len(X_test))\n",
    "    speed_anchor_seconds.append((exp1[1] + exp2[1]) / 2)\n",
    "    identity_anchor_scores.append(metrics_rules.calc_identity_rules(exp1[0], exp2[0]))\n",
    "    separability_anchor_scores.append(metrics_rules.calc_separability_rules(exp1[0]))\n",
    "\n",
    "df_t = pd.concat([\n",
    "    pd.Series(dataset_indeces, name='dataset_indeces'),\n",
    "    #pd.Series(accuracy_scores, name='accuracy_scores'),\n",
    "    pd.Series(identity_anchor_scores, name='identity_anchor_scores'),\n",
    "    pd.Series(separability_anchor_scores, name='separability_anchor_scores'),\n",
    "    pd.Series(speed_anchor_seconds, name='speed_anchor_seconds'),\n",
    "    #pd.Series(precision_scores, name='precision_scores')\n",
    "], axis=1)\n",
    "\n",
    "# df_t.to_csv('records_anchor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
