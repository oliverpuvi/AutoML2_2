{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4ff37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install skope-rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3fc7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py-ciu==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99f5db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ciu import determine_ciu\n",
    "import six\n",
    "import sys\n",
    "import os\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from skrules import SkopeRules\n",
    "import openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ec78561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New proposed separability\n",
    "\n",
    "#calculate the centroid (mean vector) of the feature values. \n",
    "#This represents the average position of each class in the feature space\n",
    "def calculate_centroids(X, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    centroids = {label: X[labels == label].mean(axis=0) for label in unique_labels}\n",
    "    return centroids\n",
    "\n",
    "#Calculate the average variance within each class. \n",
    "#This measures how spread out each class is around its centroid.\n",
    "def calculate_within_class_variance(X, labels, centroids):\n",
    "    unique_labels = np.unique(labels)\n",
    "    variances = {label: ((X[labels == label] - centroids[label])**2).mean() for label in unique_labels}\n",
    "    total_variance = np.mean(list(variances.values()))\n",
    "    return total_variance\n",
    "\n",
    "#Calculate the distance (e.g., Euclidean distance) between the centroids of each pair of classes. \n",
    "#This measures how far apart the classes are from each other.\n",
    "def calculate_between_class_separation(centroids):\n",
    "    unique_labels = list(centroids.keys())\n",
    "    separations = []\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(i+1, len(unique_labels)):\n",
    "            separation = np.linalg.norm(centroids[unique_labels[i]] - centroids[unique_labels[j]])\n",
    "            separations.append(separation)\n",
    "    avg_separation = np.mean(separations)\n",
    "    return avg_separation\n",
    "\n",
    "def calculate_separability(X, labels):\n",
    "    centroids = calculate_centroids(X, labels)\n",
    "    within_class_var = calculate_within_class_variance(X, labels, centroids)\n",
    "    between_class_sep = calculate_between_class_separation(centroids)\n",
    "    \n",
    "    if within_class_var == 0:  \n",
    "        return np.inf\n",
    "    \n",
    "    separability_score = between_class_sep / within_class_var\n",
    "    return separability_score #Higher values indicate better separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2100d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_identity(exp1, exp2):\n",
    "    dis = np.array([np.array_equal(exp1[i], exp2[i]) for i in range(len(exp1))])\n",
    "    total = dis.shape[0]\n",
    "    true = np.sum(dis)\n",
    "    score = (total - true) / total\n",
    "    return score * 100, true, total\n",
    "\n",
    "def calc_stability(exp, labels):\n",
    "    total = labels.shape[0]\n",
    "    label_values = np.unique(labels)\n",
    "    n_clusters = label_values.shape[0]\n",
    "    init = np.array([[np.average(exp[np.where(labels == i)], axis = 0)] for i in label_values]).squeeze()\n",
    "    ct = sklearn.cluster.KMeans(n_clusters = n_clusters, random_state=1, n_init=10, init = init)\n",
    "    ct.fit(exp)\n",
    "    error = np.sum(np.abs(labels-ct.labels_))\n",
    "    if error/total > 0.5:\n",
    "        error = total-error\n",
    "    return error, total\n",
    "\n",
    "def enc_exp(exp, feature_num):\n",
    "    enc_exp = np.zeros((len(exp),feature_num))\n",
    "    for i in range(len(exp)):\n",
    "        for j in range(len(exp[i])):\n",
    "            enc_exp[i][int(exp[i,j,0])] = exp[i,j,1]\n",
    "    return enc_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6a36d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(x, x_dash):\n",
    "    x = x.copy()\n",
    "    x_dash = x_dash.copy()\n",
    "    x_rand = np.random.random(x.shape[0])\n",
    "    x_new = [x[i] if x_rand[i] > 0.5 else x_dash[i] for i in range(len(x))]\n",
    "    x_dash_new = [x_dash[i] if x_rand[i] > 0.5 else x[i] for i in range(len(x))]\n",
    "    return x_new, x_dash_new\n",
    "\n",
    "def calc_trust_score(test_x, exp, m, feat_list, model):\n",
    "    total_recalls = []\n",
    "    for i in range(len(test_x)):\n",
    "        feat_score = np.zeros((len(feat_list)))\n",
    "        for _ in range(m):\n",
    "            x = test_x[i].copy()\n",
    "            x_dash = test_x[np.random.randint(0,len(test_x))].copy()\n",
    "            x_perm, x_dash_perm = permute(x, x_dash)\n",
    "            for j in range(len(feat_list)):\n",
    "                z = np.concatenate((x_perm[:j+1], x_dash_perm[j+1:]))\n",
    "                z_dash = np.concatenate((x_dash_perm[:j], x_perm[j:]))\n",
    "                p_z = model.predict_proba(np.array(z).reshape(1, -1))\n",
    "                p_z_dash = model.predict_proba(z_dash.reshape(1,-1))\n",
    "                feat_score[j] = feat_score[j] + np.linalg.norm(p_z-p_z_dash)\n",
    "        feat_score = feat_score/m\n",
    "        gold_feat_fs = np.argpartition(feat_score, -6)[-6:]\n",
    "        recall = len(set(exp[i][:6, 0]).intersection(set(gold_feat_fs)))/6\n",
    "        total_recalls.append(recall)\n",
    "    return np.mean(total_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d3360d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = \"datasets\"\n",
    "\n",
    "# Initialize empty lists to store dataframes for each file\n",
    "folder_names = []\n",
    "attribute_names_list = []\n",
    "categorical_indicator_list = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Loop through each folder in the datasets folder\n",
    "for folder_name in os.listdir(datasets_folder):\n",
    "    folder_path = os.path.join(datasets_folder, folder_name)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Construct file paths for each CSV file in the folder\n",
    "        attribute_names_path = os.path.join(folder_path, \"attribute_names.csv\")\n",
    "        categorical_indicator_path = os.path.join(folder_path, \"categorical_indicator.csv\")\n",
    "        X_path = os.path.join(folder_path, \"X.csv\")\n",
    "        y_path = os.path.join(folder_path, \"y.csv\")\n",
    "        \n",
    "        # Read each CSV file into a pandas dataframe\n",
    "        attribute_names_df = pd.read_csv(attribute_names_path)\n",
    "        categorical_indicator_df = pd.read_csv(categorical_indicator_path)\n",
    "        X_df = pd.read_csv(X_path)\n",
    "        y_df = pd.read_csv(y_path)\n",
    "        \n",
    "        # Append dataframes to the lists\n",
    "        attribute_names_list.append(attribute_names_df)\n",
    "        categorical_indicator_list.append(categorical_indicator_df)\n",
    "        X_list.append(X_df)\n",
    "        y_list.append(y_df)\n",
    "        X_list = [df.head(50) for df in X_list]\n",
    "        y_list = [df.head(50) for df in y_list]\n",
    "\n",
    "        # Save folder name to list\n",
    "        folder_names.append(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1c051e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1046 - Accuracy: 0.9\n",
      "Identity score is 60.0\n",
      "Separability score is 6.935585161831817e-06\n",
      "Speed: 1.09\n",
      "Dataset 1053 - Accuracy: 1.0\n",
      "Identity score is 10.0\n",
      "Separability score is 9.436535017662588e-05\n",
      "Speed: 5.21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ciu import determine_ciu\n",
    "\n",
    "\n",
    "def create_pipeline(X):\n",
    "    \"\"\"\n",
    "    Create a pipeline based on the types of features in X.\n",
    "    \"\"\"\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Create transformers for numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def exp_fn_ciu(xtest, model, X_train):\n",
    "    exp1 = []\n",
    "    for i in range(len(xtest)):\n",
    "        exp = determine_ciu(xtest.iloc[i:i+1], model.predict_proba, X_train.to_dict('list'), samples=1000, prediction_index=1)\n",
    "        exp_list = [[feat_list.index(i), exp.ci[i]] for i in exp.ci]\n",
    "        exp1.append(exp_list)\n",
    "    return np.array(exp1)\n",
    "\n",
    "def permute(x, x_dash):\n",
    "    x = x.copy()\n",
    "    x_dash = x_dash.copy()\n",
    "    x_rand = np.random.random(x.shape[0])\n",
    "    x_new = [x[i] if x_rand[i] > 0.5 else x_dash[i] for i in range(len(x))]\n",
    "    x_dash_new = [x_dash[i] if x_rand[i] > 0.5 else x[i] for i in range(len(x))]\n",
    "    return x_new, x_dash_new\n",
    "\n",
    "def calc_trust_score(test_x, exp, m, feat_list, model):\n",
    "    total_recalls = []\n",
    "    for i in range(len(test_x)):\n",
    "        feat_score = np.zeros((len(feat_list)))\n",
    "        for _ in range(m):\n",
    "            x = test_x[i].copy()\n",
    "            x_dash = test_x[np.random.randint(0,len(test_x))].copy()\n",
    "            x_perm, x_dash_perm = permute(x, x_dash)\n",
    "            for j in range(len(feat_list)):\n",
    "                z = np.concatenate((x_perm[:j+1], x_dash_perm[j+1:]))\n",
    "                z_dash = np.concatenate((x_dash_perm[:j], x_perm[j:]))\n",
    "                z = np.array(z).reshape(1, -1)\n",
    "                z_dash = np.array(z_dash).reshape(1, -1)\n",
    "                \n",
    "                p_z = model.predict_proba(z)\n",
    "                p_z_dash = model.predict_proba(z_dash)\n",
    "                feat_score[j] = feat_score[j] + np.linalg.norm(p_z-p_z_dash)\n",
    "        feat_score = feat_score/m\n",
    "        gold_feat_fs = np.argpartition(feat_score, -6)[-6:]\n",
    "        recall = len(set(exp[i][:6, 0]).intersection(set(gold_feat_fs)))/6\n",
    "        total_recalls.append(recall)\n",
    "    return np.mean(total_recalls)\n",
    "\n",
    "ciu_identity_scores = []\n",
    "ciu_lime_separability_scores = []\n",
    "ciu_speed_scores = []\n",
    "ciu_fidelity_scores = []\n",
    "for i in range(len(X_list)):\n",
    "    X, y = X_list[i], y_list[i].squeeze()  # Ensure y is a 1D array\n",
    "    \n",
    "    # Convert X and y to numeric if not already\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')  # 'coerce' option converts non-numeric values to NaN\n",
    "    \n",
    "    #Calculate overall separability\n",
    "    ciu_lime_separability = calculate_separability(X, y)\n",
    "    ciu_lime_separability_scores.append(ciu_lime_separability)\n",
    "    \n",
    "    preprocessor = create_pipeline(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    feat_list = X_test.columns.tolist()\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', RandomForestClassifier())])\n",
    "    \n",
    "    test_x = X_test.values\n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Dataset {folder_names[i]} - Accuracy: {accuracy_score(y_test, pipeline.predict(X_test))}\")\n",
    "    \n",
    "    #Applying bulk CIU\n",
    "    start_time = time.time()\n",
    "    bulk_ciu_result = exp_fn_ciu(X_test, pipeline, X_train)\n",
    "    end_time = time.time()\n",
    "    ciu_speed = end_time - start_time\n",
    "    ciu_speed_scores.append(ciu_speed)\n",
    "    bulk_ciu_result2 = exp_fn_ciu(X_test, pipeline, X_train)\n",
    "    \n",
    "    #Calculating identity score\n",
    "    ciu_identity = calc_identity(bulk_ciu_result, bulk_ciu_result2)\n",
    "    ciu_identity_scores.append(ciu_identity[0])\n",
    "    \n",
    "    \n",
    "    #Calculating fidelity scores\n",
    "    #ciu_fidelity = calc_trust_score(test_x, bulk_ciu_result, 5, feat_list, pipeline)\n",
    "    #ciu_fidelity_scores.append(ciu_fidelity)\n",
    "    # Calculating stability\n",
    "    #enc1 = enc_exp(bulk_ciu_result, len(feat_list))\n",
    "    #ciu_stability = calc_stability(enc1, y_test)   \n",
    "    \n",
    "    print(f\"Identity score is {ciu_identity[0]}\")\n",
    "    print(f\"Separability score is {ciu_lime_separability}\")\n",
    "    #print(f\"Fidelity score is {ciu_fidelity}\")\n",
    "    #print(f\"Stability score is {ciu_stability}\")\n",
    "    print(f\"Speed: {round(ciu_speed, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "76857715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.concat([\n",
    "    pd.Series(ciu_lime_separability_scores, name='Separability_scores'),\n",
    "    pd.Series(ciu_identity_scores, name='CIU_identity_scores'),\n",
    "    pd.Series(ciu_speed_scores, name='CIU_speed_scores')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a7dc622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.to_csv('2_records_ciu.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
